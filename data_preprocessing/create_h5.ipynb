{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#P4J library must be compiled and installed\n",
    "#mexican hatt too\n",
    "\n",
    "-march=x86-64\n",
    "python setup.py build_ext --inplace\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "dirs = glob.glob('/home/mdelafuente/SSL/out_directory/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216416/216416 [01:03<00:00, 3429.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "grab_oids = []\n",
    "for dir in tqdm(dirs, total = len(dirs)):\n",
    "    with (open(dir, \"rb\")) as openfile:\n",
    "        load = pickle.load(openfile)\n",
    "        grab_oids.append(load['oid'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "oid    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oids_df = pd.DataFrame({'oid':grab_oids}, dtype='str') \n",
    "oids_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_df = pd.read_parquet('/home/mdelafuente/forced_photometry_oids/partitions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_oids = ff_df['oid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "23577    ZTF18adldhip\n",
       "32093    ZTF19aasbgeb\n",
       "17103    ZTF18aayfbqd\n",
       "6117     ZTF19ablyzbl\n",
       "33717    ZTF18actabfv\n",
       "             ...     \n",
       "41704    ZTF19acokmuo\n",
       "41708    ZTF18abxoxih\n",
       "41716    ZTF18adbhpsf\n",
       "41719    ZTF21aamkwra\n",
       "41726    ZTF23aaqniyk\n",
       "Name: oid, Length: 175256, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_oids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1438,)\n"
     ]
    }
   ],
   "source": [
    "cross_contamination = pd.merge(ff_df,oids_df, on='oid', how='inner')['oid'].unique()\n",
    "print(cross_contamination.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZTF20acrmbzf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZTF20acrmebv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZTF20acrmhce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZTF20acrmknm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZTF20acrnjcr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216411</th>\n",
       "      <td>ZTF20abuvuel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216412</th>\n",
       "      <td>ZTF23aapxmfe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216413</th>\n",
       "      <td>ZTF20abwdjei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216414</th>\n",
       "      <td>ZTF22aarupkz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216415</th>\n",
       "      <td>ZTF22aascgpz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214978 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 oid\n",
       "0       ZTF20acrmbzf\n",
       "1       ZTF20acrmebv\n",
       "2       ZTF20acrmhce\n",
       "3       ZTF20acrmknm\n",
       "4       ZTF20acrnjcr\n",
       "...              ...\n",
       "216411  ZTF20abuvuel\n",
       "216412  ZTF23aapxmfe\n",
       "216413  ZTF20abwdjei\n",
       "216414  ZTF22aarupkz\n",
       "216415  ZTF22aascgpz\n",
       "\n",
       "[214978 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oids_df = oids_df[~oids_df['oid'].isin(cross_contamination)]\n",
    "oids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_train_val_split(df_oids: pd.DataFrame,split_frac:float):\n",
    "    \"\"\"Will create a train/validation split by splitting a pd dataframe of oids (single column dataframe where index is range of N and column contains the oids)\n",
    "\n",
    "    Args:\n",
    "        df_oids (pd.DataFrame): _description_\n",
    "    \"\"\"\n",
    "    assert split_frac < 1\n",
    "    assert df_oids.empty == False\n",
    "    assert split_frac > 0\n",
    "    validation = df_oids.sample(frac=0.2, random_state=42)  # for reproducibility\n",
    "    #print(validation)\n",
    "    train = df_oids.loc[~df_oids.index.isin(validation.index)]\n",
    "    #train = train.values.flatten().tolist()\n",
    "    #validation = validation.values.flatten().tolist()\n",
    "\n",
    "    return train,validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = create_single_train_val_split(oids_df,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "def create_h5_file(filename, data_dict):\n",
    "    \"\"\"\n",
    "    Create an HDF5 file and save data from a dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    filename (str): Name or path of the HDF5 file to create\n",
    "    data_dict (dict): Dictionary containing the data to save\n",
    "    \"\"\"\n",
    "    # Create directories if they don't exist\n",
    "    directory = os.path.dirname(filename)\n",
    "    if directory:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        for key, value in data_dict.items():\n",
    "            if not isinstance(value, np.ndarray):\n",
    "                value = np.array(value)\n",
    "            f.create_dataset(key, data=value)\n",
    "    \n",
    "    # Return the absolute path of the created file\n",
    "    return os.path.abspath(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalkeys = {}\n",
    "for i in range(5):\n",
    "    train,val = create_single_train_val_split(oids_df,0.2)\n",
    "    train = train.index.tolist()\n",
    "    train.sort()\n",
    "    val = val.index.tolist()\n",
    "    val.sort()\n",
    "   # print(train)\n",
    "    #assert not np.any(np.in1d(train.values.flatten(), val.values.flatten()))\n",
    "    trainvalkeys.update({f'train_{i}':train})\n",
    "    trainvalkeys.update({f'validation_{i}':val})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "class atat2h5:\n",
    "    def __init__(self,out_path,input_dir):\n",
    "        self.out = out_path\n",
    "        self.input = input_dir\n",
    "        pass \n",
    "    def get_oids():\n",
    "        pass\n",
    "    def establish_splits():\n",
    "        pass\n",
    "    def get_flux():\n",
    "        pass \n",
    "    def get_mask():\n",
    "        pass\n",
    "    def get_time(): \n",
    "        pass\n",
    "    def get_ft():\n",
    "        pass\n",
    "    def get_mask():\n",
    "        pass\n",
    "    def get_window_feats():\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "file_path = create_h5_file('/home/mdelafuente/SSL/final_dataset/no_contamination.h5', trainvalkeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 root level keys: ['train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "with h5py.File('/home/mdelafuente/SSL/final_dataset/no_contamination.h5', 'r') as f:\n",
    "    # Print original dictionary keys\n",
    "    \n",
    "    # Print HDF5 root level keys\n",
    "    print(\"HDF5 root level keys:\", list(f.keys()))\n",
    "    these_idx = f.get('train_0')[:100]\n",
    "    print(f.get('md_cols'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216416/216416 [01:05<00:00, 3322.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 root level keys: ['flux', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "grab_flux = [] \n",
    "for dir in tqdm(dirs, total = len(dirs)):\n",
    "    with (open(dir, \"rb\")) as openfile:\n",
    "        load = pickle.load(openfile) \n",
    "        grab_flux.append(load['flux'])\n",
    "with h5py.File('/home/mdelafuente/SSL/final_dataset/no_contamination.h5', 'r+') as f:\n",
    "    # Print original dictionary keys\n",
    "    f.create_dataset('flux',data =np.array(grab_flux).astype(float) )\n",
    "    # Print HDF5 root level keys\n",
    "    print(\"HDF5 root level keys:\", list(f.keys()))\n",
    "del grab_flux \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216416/216416 [01:03<00:00, 3385.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 root level keys: ['flux', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n"
     ]
    }
   ],
   "source": [
    "grab_time = [] \n",
    "for dir in tqdm(dirs, total = len(dirs)):\n",
    "    with (open(dir, \"rb\")) as openfile:\n",
    "        load = pickle.load(openfile) \n",
    "        grab_time.append(load['time'])\n",
    "with h5py.File('/home/mdelafuente/SSL/final_dataset/no_contamination.h5', 'r+') as f:\n",
    "    # Print original dictionary keys\n",
    "    f.create_dataset('time',data =np.array(grab_time).astype(float) )\n",
    "    # Print HDF5 root level keys\n",
    "    print(\"HDF5 root level keys:\", list(f.keys()))\n",
    "del grab_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216416/216416 [01:03<00:00, 3383.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 root level keys: ['flux', 'mask', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n"
     ]
    }
   ],
   "source": [
    "grab_mask = [] \n",
    "for dir in tqdm(dirs, total = len(dirs)):\n",
    "    with (open(dir, \"rb\")) as openfile:\n",
    "        load = pickle.load(openfile) \n",
    "        grab_mask.append(load['mask'])\n",
    "with h5py.File('/home/mdelafuente/SSL/final_dataset/no_contamination.h5', 'r+') as f:\n",
    "    # Print original dictionary keys\n",
    "    f.create_dataset('mask',data =np.array(grab_mask).astype(float) )\n",
    "    # Print HDF5 root level keys\n",
    "    print(\"HDF5 root level keys:\", list(f.keys()))\n",
    "del grab_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216416/216416 [02:22<00:00, 1517.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 root level keys: ['flux', 'ft_cols', 'mask', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n"
     ]
    }
   ],
   "source": [
    "grab_ft = [] \n",
    "for dir in tqdm(dirs, total = len(dirs)):\n",
    "    with (open(dir, \"rb\")) as openfile:\n",
    "        load = pickle.load(openfile) \n",
    "        feats = pd.DataFrame(load['ft_cols'].value.values)\n",
    "        \n",
    "        feats = feats.replace([np.inf, -np.inf], np.nan)\n",
    "        grab_ft.append(feats.values)\n",
    "with h5py.File('/home/mdelafuente/SSL/final_dataset/no_contamination.h5', 'r+') as f:\n",
    "    # Print original dictionary keys\n",
    "    f.create_dataset('ft_cols',data =np.array(grab_ft).astype(float))\n",
    "    # Print HDF5 root level keys\n",
    "    print(\"HDF5 root level keys:\", list(f.keys()))\n",
    "del grab_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216416/216416 [02:18<00:00, 1560.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 root level keys: ['flux', 'ft_cols', 'mask', 'md_cols', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n"
     ]
    }
   ],
   "source": [
    "grab_md = [] \n",
    "for dir in tqdm(dirs, total = len(dirs)):\n",
    "    with (open(dir, \"rb\")) as openfile:\n",
    "        load = pickle.load(openfile) \n",
    "        feats = pd.DataFrame(load['md_cols'].value.values)\n",
    "        \n",
    "        feats = feats.replace([np.inf, -np.inf], np.nan)\n",
    "        grab_md.append(feats.values)\n",
    "with h5py.File('/home/mdelafuente/SSL/final_dataset/no_contamination.h5', 'r+') as f:\n",
    "    # Print original dictionary keys\n",
    "    f.create_dataset('md_cols',data =np.array(grab_md).astype(float))\n",
    "    # Print HDF5 root level keys\n",
    "    print(\"HDF5 root level keys:\", list(f.keys()))\n",
    "del grab_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QT CREATE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216416/216416 [02:33<00:00, 1406.12it/s]\n",
      "/home/mdelafuente/miniconda3/envs/dataset/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1563: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a,\n"
     ]
    }
   ],
   "source": [
    "grab_md = [] \n",
    "\n",
    "for dir in tqdm(dirs, total = len(dirs)):\n",
    "    with (open(dir, \"rb\")) as openfile:\n",
    "        load = pickle.load(openfile) \n",
    "        feats = pd.DataFrame(load['md_cols'].value.values).astype(float)\n",
    "        \n",
    "\n",
    "        #feats = feats.fillna(-9999)\n",
    "        \n",
    "        feats = feats.replace([np.inf, -np.inf], np.nan)\n",
    "    grab_md.append(feats.values.flatten())\n",
    "\n",
    "    \n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "nan_mask = np.isnan(np.array(grab_md))\n",
    "\n",
    "df = pd.DataFrame(grab_md)\n",
    "qt = QuantileTransformer(\n",
    "    n_quantiles=1000, random_state=0, output_distribution=\"uniform\"\n",
    "    )  \n",
    "qt.fit(df[~nan_mask])\n",
    "df = qt.transform(df.fillna(12345)) + 0.1\n",
    "df[nan_mask] = 0.0\n",
    "df = df.reshape(df.shape[0],df.shape[1],1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216416, 6, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QT transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import h5py\n",
    "\n",
    "\n",
    "def create_ft_fold(out_dir, fold_name,seed):\n",
    "    qt = QuantileTransformer(\n",
    "    n_quantiles=1000, random_state=0, output_distribution=\"uniform\"\n",
    "    )   \n",
    "\n",
    "    with h5py.File('/home/mdelafuente/SSL/final_dataset/no_contamination.h5', 'r') as f:\n",
    "        # Print original dictionary keys\n",
    "        \n",
    "        # Print HDF5 root level keys\n",
    "        print(\"HDF5 root level keys:\", list(f.keys()))\n",
    "        these_idx = f.get(f'train_{seed}')\n",
    "        print(these_idx)\n",
    "        feats = f.get('ft_cols')[these_idx]\n",
    "    feats = feats.reshape(feats.shape[0],feats.shape[1])\n",
    "    nan_mask = np.isnan(np.array(feats))\n",
    "\n",
    "    df = pd.DataFrame(feats)\n",
    "\n",
    "    qt.fit(df[~nan_mask])\n",
    "    #df = qt.transform(df.fillna(12345)) + 0.1\n",
    "    #df[nan_mask] = 0.0\n",
    "    #df = df.reshape(df.shape[0],df.shape[1],1)\n",
    "    #feats = np.concatenate([feat for feat in collect_feats]) \n",
    "    #print(feats)\n",
    "    print()\n",
    "    \n",
    " \n",
    "    joblib.dump(qt, f'{out_dir}/{fold_name}_qt-fold-{seed}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def create_md_fold(out_dir, fold_name,seed):\n",
    "    qt = QuantileTransformer(\n",
    "    n_quantiles=1000, random_state=0, output_distribution=\"uniform\"\n",
    "    )   \n",
    "    collect_feats = []\n",
    "\n",
    "    with h5py.File('/home/mdelafuente/SSL/final_dataset/no_contamination.h5', 'r') as f:\n",
    "        # Print original dictionary keys\n",
    "        \n",
    "        # Print HDF5 root level keys\n",
    "        print(\"HDF5 root level keys:\", list(f.keys()))\n",
    "        these_idx = f.get(f'train_{seed}')\n",
    "        #print(these_idx)\n",
    "        feats = f.get('md_cols')[these_idx] \n",
    "    feats = feats.reshape(feats.shape[0],feats.shape[1])\n",
    "    nan_mask = np.isnan(np.array(feats))\n",
    "\n",
    "    df = pd.DataFrame(feats)\n",
    "\n",
    "    qt.fit(df[~nan_mask])\n",
    "    print()\n",
    " \n",
    "    joblib.dump(qt, f'{out_dir}/{fold_name}_qt-fold-{seed}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 root level keys: ['flux', 'ft_cols', 'mask', 'md_cols', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n",
      "<HDF5 dataset \"train_0\": shape (171982,), type \"<i8\">\n",
      "\n",
      "HDF5 root level keys: ['flux', 'ft_cols', 'mask', 'md_cols', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdelafuente/miniconda3/envs/dataset/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1563: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HDF5 root level keys: ['flux', 'ft_cols', 'mask', 'md_cols', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n",
      "<HDF5 dataset \"train_1\": shape (171982,), type \"<i8\">\n",
      "\n",
      "HDF5 root level keys: ['flux', 'ft_cols', 'mask', 'md_cols', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdelafuente/miniconda3/envs/dataset/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1563: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HDF5 root level keys: ['flux', 'ft_cols', 'mask', 'md_cols', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n",
      "<HDF5 dataset \"train_2\": shape (171982,), type \"<i8\">\n",
      "\n",
      "HDF5 root level keys: ['flux', 'ft_cols', 'mask', 'md_cols', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdelafuente/miniconda3/envs/dataset/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1563: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HDF5 root level keys: ['flux', 'ft_cols', 'mask', 'md_cols', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n",
      "<HDF5 dataset \"train_3\": shape (171982,), type \"<i8\">\n",
      "\n",
      "HDF5 root level keys: ['flux', 'ft_cols', 'mask', 'md_cols', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdelafuente/miniconda3/envs/dataset/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1563: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HDF5 root level keys: ['flux', 'ft_cols', 'mask', 'md_cols', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n",
      "<HDF5 dataset \"train_4\": shape (171982,), type \"<i8\">\n",
      "\n",
      "HDF5 root level keys: ['flux', 'ft_cols', 'mask', 'md_cols', 'time', 'train_0', 'train_1', 'train_2', 'train_3', 'train_4', 'validation_0', 'validation_1', 'validation_2', 'validation_3', 'validation_4']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdelafuente/miniconda3/envs/dataset/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1563: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a,\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    trans_feat = create_ft_fold('/home/mdelafuente/SSL/final_dataset/','ft',seed=i)\n",
    "    trans_feat = create_md_fold('/home/mdelafuente/SSL/final_dataset/','md',seed=i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
