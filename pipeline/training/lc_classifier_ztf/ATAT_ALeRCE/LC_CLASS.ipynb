{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import glob\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from src.layers.classifiers import TokenClassifier, MixedClassifier\n",
    "\n",
    "import h5py\n",
    "import sys\n",
    "import umap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {\n",
    "    \"AGN\": 0,\n",
    "    \"QSO\": 1,\n",
    "    \"EA\": 2,\n",
    "    \"YSO\": 3,\n",
    "    \"SNIa\": 4,\n",
    "    \"CV/Nova\": 5,\n",
    "    \"RRLc\": 6,\n",
    "    \"RSCVn\": 7,\n",
    "    \"Blazar\": 8,\n",
    "    \"SNII\": 9,\n",
    "    \"EB/EW\": 10,\n",
    "    \"LPV\": 11,\n",
    "    \"CEP\": 12,\n",
    "    \"RRLab\": 13,\n",
    "    \"Periodic-Other\": 14,\n",
    "    \"DSCT\": 15,\n",
    "    \"SNIbc\": 16,\n",
    "    \"SLSN\": 17,\n",
    "    \"TDE\": 18,\n",
    "    \"SNIIb\": 19,\n",
    "    \"SNIIn\": 20,\n",
    "    \"Microlensing\": 21\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/home/mdelafuente/'#sys.argv[2]\n",
    "\n",
    "device = torch.device(f\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "h5_file = h5py.File(\n",
    "    f\"{path}/ORIGINAL/elasticc_dataset_update.h5\", \"r\"\n",
    ")\n",
    "\n",
    "abs_path = '/home/mdelafuente/batch_processing/pipeline/training/lc_classifier_ztf/ATAT_ALeRCE/results/ZTF_ff/LC/NEWfinenotusingsampler/MTA/'\n",
    "path_args = glob.glob(f\"{abs_path}*args*\")[0]\n",
    "print(path_args)\n",
    "import yaml\n",
    "with open(path_args, 'r') as file:\n",
    "    args = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initializing models') \n",
    "from src.layers.cATAT import LightCurveClassifier\n",
    "model = LightCurveClassifier(**args) \n",
    "\n",
    "\n",
    "print('Loading checkpoints')\n",
    "checkpoint_path_clip = glob.glob(\n",
    "    f\"{abs_path}*my_best_checkpoint*\"\n",
    ") \n",
    "print(checkpoint_path_clip[-1])\n",
    "checkpoint_clip = torch.load(checkpoint_path_clip[-1], map_location=torch.device('cuda:2'))\n",
    "od_atat = OrderedDict() \n",
    "\n",
    "for key in checkpoint_clip[\"state_dict\"].keys(): \n",
    "        #print(key)\n",
    "         \n",
    "        if 'model' in key:\n",
    "            od_atat[key.replace(\"model.\", \"\")] = checkpoint_clip[\"state_dict\"][key]\n",
    "\n",
    "model.load_state_dict(od_atat,strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "from src.data.modules.LitData  import LitData\n",
    "\n",
    "\n",
    "args['general']['data_root'] = 'data/datasets/ZTF_ff/final/LC_MD_FEAT_240627_windows_200_12'\n",
    "args['general']['data_root'] = 'data/datasets/ZTF_ff/final/LC_MD_FEAT_240627_windows_200_12'\n",
    "args['general']['use_sampler'] = False\n",
    "pl_datal = LitData(**args['general'])\n",
    "dataloader = pl_datal.val_dataloader() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = None\n",
    "preds_out = None\n",
    "\n",
    "model.eval().to(device = 'cuda:2')  \n",
    "\n",
    "for b1 in tqdm(dataloader):\n",
    "\n",
    "    #print(b1.keys())\n",
    "    b1 = {k: b1[k].float().to(device = 'cuda:2') for k in b1.keys()}\n",
    "\n",
    "    lc_emb = model(**b1)  \n",
    "   \n",
    "    t = b1['labels']\n",
    "\n",
    "\n",
    "    preds_out = (\n",
    "        np.concatenate([preds_out, torch.argmax(lc_emb, axis=1).cpu().numpy()])\n",
    "        if preds_out is not None\n",
    "        else torch.argmax(lc_emb, axis=1).cpu().detach().numpy()\n",
    "    )\n",
    "\n",
    "    target = (\n",
    "        np.concatenate([target, t.cpu().detach().numpy()])\n",
    "        if target is not None\n",
    "        else t.cpu().numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = preds_out\n",
    "classification = classification_report(target, results, target_names=list(dict.keys()),digits = 4)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from utils.plottinglib import elasticc_confusion_matrix\n",
    "\n",
    "out_metrics_balto = classification_report(\n",
    "    target, results, target_names=list(dict.keys()), output_dict=True\n",
    ")[\"macro avg\"]\n",
    "\n",
    "template_balto = \"\"\n",
    "\n",
    "for key in out_metrics_balto.keys():\n",
    "    template_balto += \" {} : {:.3f} \".format(key.upper(), out_metrics_balto[key])\n",
    "fig, axes = plt.subplots(1, 1, figsize=(12, 12))\n",
    "elasticc_confusion_matrix(\n",
    "    y_true=target,\n",
    "    y_pred=results,\n",
    "    classes= np.array(list(dict.keys())),\n",
    "    ax=axes,\n",
    "    normalize=True,\n",
    "    title=f\" FClassifier Results [TEST] \\n\\n {template_balto}\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
