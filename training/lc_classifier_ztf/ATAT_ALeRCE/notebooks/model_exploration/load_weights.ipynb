{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dmoreno/ATAT_ZTF/elasticc_one\n"
     ]
    }
   ],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.LitATAT import LitATAT\n",
    "from src.layers import ATAT\n",
    "\n",
    "import logging\n",
    "import glob\n",
    "import torch\n",
    "import yaml\n",
    "import copy\n",
    "\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler_checkpoint(path, args):\n",
    "    od_ = OrderedDict()\n",
    "    logging.info(\"Loading model from checkpoint ...\")\n",
    "    checkpoint_ = torch.load(path)\n",
    "    for key in checkpoint_[\"state_dict\"].keys():\n",
    "        od_[key.replace(\"atat.\", \"\")] = checkpoint_[\"state_dict\"][key]\n",
    "    logging.info(\"New keys formated ...\")\n",
    "    model = ATAT(**args)\n",
    "    logging.info(\"Build ATAT  model\")\n",
    "    try:\n",
    "        model.load_state_dict(od_, strict=False)\n",
    "        logging.info(\"All keys matched\")\n",
    "    except RuntimeError as e:\n",
    "        logging.error(f\"Error loading model state dict: {e}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def handler_ckpt_path(path):\n",
    "    out_path = glob.glob(path + \"/*.ckpt\")[0]\n",
    "    return out_path\n",
    "\n",
    "def load_yaml(path):\n",
    "    with open(path, 'r') as file:\n",
    "        args = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitATAT(\n",
       "  (atat): ATAT(\n",
       "    (time_encoder): TimeHandler(\n",
       "      (time_encoders): ModuleList(\n",
       "        (0): TimeFilmModified(\n",
       "          (linear_proj): Sequential(\n",
       "            (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TimeFilmModified(\n",
       "          (linear_proj): Sequential(\n",
       "            (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): TimeFilmModified(\n",
       "          (linear_proj): Sequential(\n",
       "            (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): TimeFilmModified(\n",
       "          (linear_proj): Sequential(\n",
       "            (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): TimeFilmModified(\n",
       "          (linear_proj): Sequential(\n",
       "            (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): TimeFilmModified(\n",
       "          (linear_proj): Sequential(\n",
       "            (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transformer_lc): Transformer(\n",
       "      (stacked_transformers): ModuleList(\n",
       "        (0): TransformerEncoder(\n",
       "          (layer_norm): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "            (1): LayerNorm()\n",
       "          )\n",
       "          (attn_forward): MultiheadAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "            )\n",
       "            (last): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=192, out_features=384, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=384, out_features=192, bias=True)\n",
       "              (4): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): TransformerEncoder(\n",
       "          (layer_norm): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "            (1): LayerNorm()\n",
       "          )\n",
       "          (attn_forward): MultiheadAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "            )\n",
       "            (last): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=192, out_features=384, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=384, out_features=192, bias=True)\n",
       "              (4): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): TransformerEncoder(\n",
       "          (layer_norm): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "            (1): LayerNorm()\n",
       "          )\n",
       "          (attn_forward): MultiheadAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "            )\n",
       "            (last): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=192, out_features=384, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=384, out_features=192, bias=True)\n",
       "              (4): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier_lc): TokenClassifier(\n",
       "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (output_layer): Linear(in_features=192, out_features=20, bias=True)\n",
       "    )\n",
       "    (token_lc): Token()\n",
       "  )\n",
       "  (train_acc): Accuracy()\n",
       "  (train_f1s): F1Score()\n",
       "  (train_rcl): Recall()\n",
       "  (valid_acc): Accuracy()\n",
       "  (valid_f1s): F1Score()\n",
       "  (valid_rcl): Recall()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_model_1 = 'results/ELASTICC_1/LC/lc_tm'\n",
    "\n",
    "args_1 = load_yaml('{}/args.yaml'.format(path_model_1))\n",
    "\n",
    "pl_model_1 = LitATAT(**args_1)\n",
    "\n",
    "pl_model_1.atat = handler_checkpoint(\n",
    "            handler_ckpt_path(path_model_1), args=args_1\n",
    "        )\n",
    "\n",
    "pl_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_1['general']['num_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': {'embedding_size': 192,\n",
       "  'embedding_size_sub': 384,\n",
       "  'num_heads': 4,\n",
       "  'num_encoders': 3,\n",
       "  'Tmax': 1500,\n",
       "  'num_harmonics': 64,\n",
       "  'encoder_type': 'Linear',\n",
       "  'max_pool_kernel': 5,\n",
       "  'cnn_kernel': 5,\n",
       "  'pe_type': 'tm',\n",
       "  'num_bands': 6},\n",
       " 'ft': {'embedding_size': 128,\n",
       "  'embedding_size_sub': 256,\n",
       "  'num_heads': 4,\n",
       "  'num_encoders': 3,\n",
       "  'encoder_type': 'Linear',\n",
       "  'length_size': 0,\n",
       "  'list_time_to_eval': None},\n",
       " 'general': {'experiment_type': 'lc',\n",
       "  'experiment_name': 'lc_tm',\n",
       "  'name_dataset': 'elasticc_1',\n",
       "  'data_root': 'data/final/ELASTICC_1/LC_MD_FEAT',\n",
       "  'use_lightcurves': True,\n",
       "  'use_metadata': False,\n",
       "  'use_features': False,\n",
       "  'batch_size': 256,\n",
       "  'num_epochs': 10000,\n",
       "  'patience': 3,\n",
       "  'lr': 0.001,\n",
       "  'use_mask_detection': False,\n",
       "  'use_time_nondetection': False,\n",
       "  'force_online_opt': False,\n",
       "  'online_opt_tt': False,\n",
       "  'use_QT': False,\n",
       "  'load_pretrained_model': False,\n",
       "  'src_checkpoint': '.',\n",
       "  'use_augmented_dataset': False,\n",
       "  'cambiar_clf': False,\n",
       "  'num_classes': 22}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_2 = copy.copy(args_1)\n",
    "args_2['general']['num_classes'] = 22\n",
    "args_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error loading model state dict: Error(s) in loading state_dict for ATAT:\n",
      "\tsize mismatch for classifier_lc.output_layer.weight: copying a param with shape torch.Size([20, 192]) from checkpoint, the shape in current model is torch.Size([22, 192]).\n",
      "\tsize mismatch for classifier_lc.output_layer.bias: copying a param with shape torch.Size([20]) from checkpoint, the shape in current model is torch.Size([22]).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LitATAT(\n",
       "  (atat): ATAT(\n",
       "    (time_encoder): TimeHandler(\n",
       "      (time_encoders): ModuleList(\n",
       "        (0): TimeFilmModified(\n",
       "          (linear_proj): Sequential(\n",
       "            (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TimeFilmModified(\n",
       "          (linear_proj): Sequential(\n",
       "            (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): TimeFilmModified(\n",
       "          (linear_proj): Sequential(\n",
       "            (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): TimeFilmModified(\n",
       "          (linear_proj): Sequential(\n",
       "            (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): TimeFilmModified(\n",
       "          (linear_proj): Sequential(\n",
       "            (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): TimeFilmModified(\n",
       "          (linear_proj): Sequential(\n",
       "            (0): Linear(in_features=1, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transformer_lc): Transformer(\n",
       "      (stacked_transformers): ModuleList(\n",
       "        (0): TransformerEncoder(\n",
       "          (layer_norm): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "            (1): LayerNorm()\n",
       "          )\n",
       "          (attn_forward): MultiheadAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "            )\n",
       "            (last): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=192, out_features=384, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=384, out_features=192, bias=True)\n",
       "              (4): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): TransformerEncoder(\n",
       "          (layer_norm): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "            (1): LayerNorm()\n",
       "          )\n",
       "          (attn_forward): MultiheadAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "            )\n",
       "            (last): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=192, out_features=384, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=384, out_features=192, bias=True)\n",
       "              (4): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): TransformerEncoder(\n",
       "          (layer_norm): ModuleList(\n",
       "            (0): LayerNorm()\n",
       "            (1): LayerNorm()\n",
       "          )\n",
       "          (attn_forward): MultiheadAttention(\n",
       "            (linears): ModuleList(\n",
       "              (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "            )\n",
       "            (last): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (feed_forward): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): Linear(in_features=192, out_features=384, bias=True)\n",
       "              (1): GELU()\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=384, out_features=192, bias=True)\n",
       "              (4): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier_lc): TokenClassifier(\n",
       "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      (output_layer): Linear(in_features=192, out_features=22, bias=True)\n",
       "    )\n",
       "    (token_lc): Token()\n",
       "  )\n",
       "  (train_acc): Accuracy()\n",
       "  (train_f1s): F1Score()\n",
       "  (train_rcl): Recall()\n",
       "  (valid_acc): Accuracy()\n",
       "  (valid_f1s): F1Score()\n",
       "  (valid_rcl): Recall()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_model_2 = LitATAT(**args_2)\n",
    "\n",
    "pl_model_2.atat = handler_checkpoint(\n",
    "            handler_ckpt_path(path_model_1), args=args_2\n",
    "        )\n",
    "pl_model_2.atat.change_clf(args_2['general'][\"num_classes\"])\n",
    "pl_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los pesos de atat.time_encoder.time_encoders.0.alpha_sin son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.0.alpha_cos son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.0.beta_sin son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.0.beta_cos son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.0.ar son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.0.linear_proj.0.weight son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.0.linear_proj.0.bias son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.1.alpha_sin son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.1.alpha_cos son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.1.beta_sin son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.1.beta_cos son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.1.ar son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.1.linear_proj.0.weight son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.1.linear_proj.0.bias son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.2.alpha_sin son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.2.alpha_cos son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.2.beta_sin son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.2.beta_cos son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.2.ar son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.2.linear_proj.0.weight son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.2.linear_proj.0.bias son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.3.alpha_sin son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.3.alpha_cos son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.3.beta_sin son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.3.beta_cos son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.3.ar son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.3.linear_proj.0.weight son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.3.linear_proj.0.bias son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.4.alpha_sin son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.4.alpha_cos son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.4.beta_sin son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.4.beta_cos son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.4.ar son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.4.linear_proj.0.weight son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.4.linear_proj.0.bias son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.5.alpha_sin son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.5.alpha_cos son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.5.beta_sin son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.5.beta_cos son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.5.ar son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.5.linear_proj.0.weight son iguales.\n",
      "Los pesos de atat.time_encoder.time_encoders.5.linear_proj.0.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.layer_norm.0.a_2 son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.layer_norm.0.b_2 son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.layer_norm.1.a_2 son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.layer_norm.1.b_2 son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.attn_forward.linears.0.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.attn_forward.linears.0.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.attn_forward.linears.1.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.attn_forward.linears.1.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.attn_forward.linears.2.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.attn_forward.linears.2.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.attn_forward.last.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.attn_forward.last.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.feed_forward.net.0.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.feed_forward.net.0.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.feed_forward.net.3.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.0.feed_forward.net.3.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.layer_norm.0.a_2 son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.layer_norm.0.b_2 son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.layer_norm.1.a_2 son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.layer_norm.1.b_2 son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.attn_forward.linears.0.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.attn_forward.linears.0.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.attn_forward.linears.1.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.attn_forward.linears.1.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.attn_forward.linears.2.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.attn_forward.linears.2.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.attn_forward.last.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.attn_forward.last.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.feed_forward.net.0.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.feed_forward.net.0.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.feed_forward.net.3.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.1.feed_forward.net.3.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.layer_norm.0.a_2 son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.layer_norm.0.b_2 son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.layer_norm.1.a_2 son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.layer_norm.1.b_2 son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.attn_forward.linears.0.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.attn_forward.linears.0.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.attn_forward.linears.1.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.attn_forward.linears.1.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.attn_forward.linears.2.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.attn_forward.linears.2.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.attn_forward.last.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.attn_forward.last.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.feed_forward.net.0.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.feed_forward.net.0.bias son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.feed_forward.net.3.weight son iguales.\n",
      "Los pesos de atat.transformer_lc.stacked_transformers.2.feed_forward.net.3.bias son iguales.\n",
      "Los pesos de atat.classifier_lc.norm.weight difieren.\n",
      "La norma de la diferencia para atat.classifier_lc.norm.weight es: 9.511879920959473\n",
      "Los pesos de atat.classifier_lc.norm.bias difieren.\n",
      "La norma de la diferencia para atat.classifier_lc.norm.bias es: 1.1033961772918701\n",
      "Los pesos de atat.classifier_lc.output_layer.weight difieren.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (20) must match the size of tensor b (22) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLos pesos de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m difieren.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Si deseas saber la magnitud de la diferencia, puedes calcular una métrica como la norma de la diferencia\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m difference \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mstate_dict_1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_dict_2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLa norma de la diferencia para \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m es: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdifference\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (20) must match the size of tensor b (22) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# Asumimos que pl_model_1 y pl_model_2 son tus dos modelos a comparar\n",
    "state_dict_1 = pl_model_1.state_dict()\n",
    "state_dict_2 = pl_model_2.state_dict()\n",
    "\n",
    "# Verificar si ambos modelos tienen la misma arquitectura\n",
    "if state_dict_1.keys() != state_dict_2.keys():\n",
    "    print(\"Los modelos tienen arquitecturas diferentes y no pueden compararse directamente.\")\n",
    "else:\n",
    "    # Iterar sobre cada clave en el state_dict para comparar los pesos\n",
    "    for key in state_dict_1:\n",
    "        if torch.equal(state_dict_1[key], state_dict_2[key]):\n",
    "            print(f\"Los pesos de {key} son iguales.\")\n",
    "        else:\n",
    "            print(f\"Los pesos de {key} difieren.\")\n",
    "\n",
    "            # Si deseas saber la magnitud de la diferencia, puedes calcular una métrica como la norma de la diferencia\n",
    "            difference = torch.norm(state_dict_1[key] - state_dict_2[key]).item()\n",
    "            print(f\"La norma de la diferencia para {key} es: {difference}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
